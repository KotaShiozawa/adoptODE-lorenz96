{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import config\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from adoptODE import dataset_adoptODE, train_adoptODE, simple_simulation\n",
    "\n",
    "config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "def lorenz96(**kwargs_sys):\n",
    "    vars = kwargs_sys['vars']\n",
    "    vars_local = kwargs_sys['vars_local']\n",
    "\n",
    "    @jit\n",
    "    def eom(y, t, params, iparams, exparams):\n",
    "        p = params['p']\n",
    "        x = jnp.array([y[v] for v in vars])\n",
    "        dx = jnp.array(jnp.roll(x, 1)*(jnp.roll(x, -1) - jnp.roll(x, 2)) - x + p)\n",
    "        return dict(zip(vars, dx))\n",
    "\n",
    "    @jit\n",
    "    def loss(ys, params, iparams, exparams, targets):\n",
    "        ys_local = {key: ys[key] for key in vars_local}\n",
    "        targets_local = {key: targets[key] for key in vars_local}\n",
    "        flat_fit = ravel_pytree(ys_local)[0]\n",
    "        flat_target = ravel_pytree(targets_local)[0]\n",
    "        return jnp.nanmean((flat_fit-flat_target)**2)\n",
    "   \n",
    "    def gen_params():\n",
    "        return {}, {}, {}\n",
    "\n",
    "    def gen_y0():\n",
    "        y = kwargs_sys['init']\n",
    "        return dict(zip(vars, y))\n",
    "\n",
    "    return eom, loss, gen_params, gen_y0, {}\n",
    "\n",
    "class Prob_density_vars:\n",
    "    def __init__(self, rng, measured):\n",
    "        self.rng = rng\n",
    "        self.measured = measured\n",
    "    \n",
    "    def __call__(self):\n",
    "        return rng.choice(self.measured, 1)\n",
    "\n",
    "class Uniform:\n",
    "    def __init__(self, rng, range):\n",
    "        self.rng = rng\n",
    "        self.range = range\n",
    "    \n",
    "    def __call__(self):\n",
    "        return np.array([rng.uniform(self.range[0], self.range[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iguesses = 10\n",
    "D = 120\n",
    "every = 6\n",
    "gen_iguess_method = 'uniform'\n",
    "iguess_range = [-1, 4]\n",
    "\n",
    "N = 10000\n",
    "len_segs = 100\n",
    "trans = 1000 \n",
    "dt = 0.01\n",
    "p = 8.17\n",
    "epochs = 3000\n",
    "lr = 0.0\n",
    "lr_y0 = 0.01\n",
    "seed = 0\n",
    "\n",
    "rng = np.random.default_rng(seed=seed)\n",
    "vars = ['x'+str(i+1).zfill(3) for i in range(D)]\n",
    "vars_measured = ['x'+str(i+1).zfill(3) for i in range(D) if i%every==0]\n",
    "vars_unmeasured = sorted(list(set(vars)-set(vars_measured)))\n",
    "vars_local_list = zip(vars_measured, vars_measured[1:] + vars_measured[:1])\n",
    "\n",
    "# Setting up system and training properties\n",
    "kwargs_sys = {'N_sys':1, 'vars':vars, 'init':rng.random(D)}\n",
    "kwargs_sys['vars_local'] = None\n",
    "t_all = jnp.arange(0, (N+trans)*dt, dt)\n",
    "t_evals = jnp.arange(0, len_segs*dt, dt)\n",
    "kwargs_adoptODE = {'epochs':epochs, 'lr':lr, 'lr_y0':lr_y0}\n",
    "\n",
    "# Generate entire time series\n",
    "true = simple_simulation(lorenz96, t_all, kwargs_sys, kwargs_adoptODE, params={'p': p})\n",
    "true = np.array([true.ys[v][0][trans:] for v in vars])\n",
    "kwargs_sys['init'] = true[:, 0]\n",
    "\n",
    "# Initial guess generation method\n",
    "if gen_iguess_method == \"prob_density_vars\": # drow from the probability density of Lorenz96 variables\n",
    "    gen_iguess = Prob_density_vars(rng, true[::every, :].flatten())\n",
    "elif gen_iguess_method == \"uniform\": # drow from a uniform distribution\n",
    "    gen_iguess = Uniform(rng, iguess_range)\n",
    "\n",
    "# Generate training data\n",
    "dataset = simple_simulation(lorenz96, t_evals, kwargs_sys, kwargs_adoptODE, params={'p': p}, params_train={'p': p})\n",
    "ys_true = copy.deepcopy(dataset.ys)\n",
    "for v in vars_unmeasured:\n",
    "    dataset.ys[v] = dataset.ys[v]*jnp.nan\n",
    "\n",
    "\n",
    "mse_measured = np.zeros((int(D/every), num_iguesses))\n",
    "estimated_init = np.zeros((D, int(D/every)*num_iguesses))\n",
    "for i, vars_local in enumerate(vars_local_list):\n",
    "    # Estimate unmeasured variables between vars_local[0] and vars_local[1]\n",
    "    kwargs_sys['vars_local'] = vars_local\n",
    "    dataset = dataset_adoptODE(lorenz96, dataset.ys, t_evals, kwargs_sys, kwargs_adoptODE, true_y0=dataset.y0, params_train={'p': p}, true_params={'p': p})\n",
    "    for j in range(num_iguesses):\n",
    "        for v in vars:\n",
    "            if v in vars_measured:\n",
    "                dataset.y0_train[v] = dataset.y0[v]\n",
    "            else:\n",
    "                dataset.y0_train[v] = gen_iguess()\n",
    "\n",
    "        params_final, losses, errors, params_history = train_adoptODE(dataset, print_interval=100, save_interval=1)\n",
    "        mse_measured[i, j] = np.mean((np.array([dataset.ys_sol[v].flatten() for v in vars_local]) - np.array([ys_true[v].flatten() for v in vars_local]))**2)\n",
    "        estimated_init[:, i*num_iguesses+j] = ravel_pytree(dataset.y0_train)[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
